{"cells":[{"cell_type":"code","source":["# @title 75. Advanced Multimodal AI Project: Setup & Configuration (Cell 1 - FIXED)\n","# =========================================================\n","# SETUP AND IMPORTS\n","# =========================================================\n","print(\"Setting up environment...\")\n","!pip install -q tensorflow transformers numpy\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Layer # Needed for HybridSynapseLayer\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n","from tensorflow.keras.applications import ResNet50\n","import numpy as np\n","import traceback\n","\n","# --- CONFIGURATION (Global Settings) ---\n","IMAGE_SHAPE = (224, 224, 3)\n","DATA_INPUT_SIZE = 50\n","NUM_CLASSES = 10\n","FRONTAL_LOBE_UNITS = 256\n","TRAINING_BATCH_SIZE = 32 # Crucial for padding/static variable shape\n","\n","# =========================================================\n","# 🛑 CRITICAL FIX: MISSING MULTIMODAL CONSTANTS\n","# These variables MUST be defined here for Cells 3, 5, and 7 to work.\n","# =========================================================\n","TS_STEPS = 10     # Time steps for RNN inputs (e.g., LSTMs)\n","TS_DIM = 1        # Feature dimension per time step\n","SEQ_LEN = 20      # Sequence length for text/attention inputs\n","SEQ_DIM = 10      # Embedding dimension per sequence step\n","GRAPH_DIM = 4     # Used for the flattened Graph input (4x4 = 16 features)\n","# =========================================================\n","\n","print(\"Configuration complete. Constants are defined.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JBBGJBo9la1P","executionInfo":{"status":"ok","timestamp":1759483821468,"user_tz":-330,"elapsed":16555,"user":{"displayName":"Deepak Soni","userId":"07768935430812623563"}},"outputId":"a7d11d8b-2f4d-488c-d1b9-e9de44670a70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Setting up environment...\n","Configuration complete. Constants are defined.\n"]}]},{"cell_type":"code","source":["# @title 75. Advanced Multimodal AI Project: Non-ANN Components (Cell 2)\n","# =========================================================\n","# NON-ANN COMPONENTS (Symbolic AI and SNN)\n","# =========================================================\n","import numpy as np\n","import tensorflow as tf\n","# NOTE: No MATLAB code is required.\n","\n","class RuleBasedReasoner:\n","    \"\"\"Symbolic AI for rule-based refinement.\"\"\"\n","    def __init__(self, confidence_threshold=0.95):\n","        self.confidence_threshold = confidence_threshold\n","\n","    def apply_rules(self, classification_probabilities, input_text):\n","        max_prob = np.max(classification_probabilities)\n","\n","        # Rule 1: High confidence ML decision\n","        if max_prob >= self.confidence_threshold:\n","            return \"ML_TRUST\", np.argmax(classification_probabilities)\n","\n","        # Rule 2: Symbolic Override (e.g., critical safety alert)\n","        if \"deny\" in input_text.lower() and max_prob < 0.8:\n","            # Assuming class '1' means 'safety denial' or 'override action'\n","            return \"SYMBOLIC_OVERRIDE\", 1\n","\n","        # Default ML Decision\n","        return \"ML_DEFAULT\", np.argmax(classification_probabilities)\n","\n","class LIFNeuron:\n","    \"\"\"Spiking Neural Network (LIF) component.\"\"\"\n","    def __init__(self, R=1.0, C=10.0, V_th=0.5):\n","        self.R = R\n","        self.C = C\n","        self.V_th = V_th\n","        self.tau = R * C\n","        self.V = 0.0\n","        self.t_ref = 0\n","\n","    def step(self, I_in, dt=1.0, refractory_period=5):\n","        spike = 0\n","        if self.t_ref > 0:\n","            self.V = 0.0\n","            self.t_ref -= 1\n","        else:\n","            # LIF Model: dV/dt = (-V + R*I)/tau\n","            dV = ((-self.V + self.R * I_in) / self.tau) * dt\n","            self.V += dV\n","            if self.V >= self.V_th:\n","                spike = 1\n","                self.V = 0.0\n","                self.t_ref = refractory_period\n","        return spike, self.V\n","\n","    def process_features(self, feature_vector):\n","        # Scale the feature vector sum to act as input current (I_in)\n","        I_in = np.sum(feature_vector)\n","        total_spikes = 0\n","        self.V = 0.0\n","        # Simulate 10 time steps\n","        for t in range(10):\n","            # Divide I_in by time steps to model continuous input\n","            spike, _ = self.step(I_in / 10.0)\n","            total_spikes += spike\n","        return total_spikes"],"metadata":{"id":"TFBUvjYPbuRs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 75. Advanced Multimodal AI Project: 21-Stream Model & Hybrid Layers (Cell 3 - FIXED)\n","# =========================================================\n","# CUSTOM LAYERS, FRONTAL LOBE LOGIC, and 21-STREAM ARCHITECTURE\n","# =========================================================\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Concatenate, LSTM, Attention, Dropout, GRU, Conv2D, MaxPooling2D, Flatten, Reshape, TimeDistributed, Embedding, GlobalAveragePooling1D # Added GlobalAveragePooling1D\n","from tensorflow.keras.applications import ResNet50\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Layer\n","import numpy as np\n","\n","# --- Custom Layer Definitions (Hybrid & PFC) ---\n","class HybridSynapseLayer(Layer):\n","    \"\"\"Implements synaptic plasticity and standard Keras function.\"\"\"\n","    def __init__(self, units, plasticity_rate=0.01, decay_factor=0.8, **kwargs):\n","        super().__init__(**kwargs)\n","        self.units = units\n","        self.plasticity_rate = plasticity_rate\n","        self.decay_factor = decay_factor\n","        self.input_activity_var = None\n","        self.output_activity_var = None\n","    def build(self, input_shape):\n","        input_dim = input_shape[-1]\n","        self.w = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True, name='kernel')\n","        self.b = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True, name='bias')\n","        # Requires TRAINING_BATCH_SIZE (defined in Cell 1) to be defined\n","        self.input_activity_var = self.add_weight(name='input_activity_var', shape=(TRAINING_BATCH_SIZE, input_dim), initializer='zeros', trainable=False)\n","        self.output_activity_var = self.add_weight(name='output_activity_var', shape=(TRAINING_BATCH_SIZE, self.units), initializer='zeros', trainable=False)\n","        self.built = True\n","    def call(self, inputs):\n","        output = tf.matmul(inputs, self.w) + self.b\n","        output_act = tf.nn.relu(output)\n","        current_batch_size = tf.shape(inputs)[0]\n","        padding_size = TRAINING_BATCH_SIZE - current_batch_size\n","        padding_input = tf.zeros([padding_size, tf.shape(inputs)[1]], dtype=inputs.dtype)\n","        padding_output = tf.zeros([padding_size, tf.shape(output_act)[1]], dtype=output_act.dtype)\n","        padded_inputs = tf.concat([inputs, padding_input], axis=0)\n","        padded_outputs = tf.concat([output_act, padding_output], axis=0)\n","        # Store activity for plasticity calculation in train_step\n","        self.input_activity_var.assign(padded_inputs)\n","        self.output_activity_var.assign(padded_outputs)\n","        return output_act\n","    @tf.function\n","    def calculate_plasticity_change(self):\n","        input_act_full = self.input_activity_var\n","        output_act_full = self.output_activity_var\n","        input_act_reshaped = tf.expand_dims(input_act_full, axis=-1)\n","        output_act_reshaped = tf.expand_dims(output_act_full, axis=1)\n","        # Hebbian Term: LTP\n","        hebbian_term = tf.reduce_mean(tf.matmul(input_act_reshaped, output_act_reshaped), axis=0)\n","        ltp_change = self.plasticity_rate * hebbian_term\n","        # Weight Decay Term: LTD\n","        ltd_change = self.plasticity_rate * self.decay_factor * self.w\n","        delta_plasticity = ltp_change - ltd_change\n","        return delta_plasticity, tf.reduce_sum(tf.abs(delta_plasticity))\n","\n","class ExecutiveGatingLayer(Layer):\n","    \"\"\"PFC-like working memory layer.\"\"\"\n","    def __init__(self, units, context_decay=0.9, context_learn_rate=0.1, activation='relu', **kwargs):\n","        super().__init__(**kwargs)\n","        self.units = units\n","        self.context_decay = context_decay\n","        self.context_learn_rate = context_learn_rate\n","        self.activation = tf.keras.activations.get(activation)\n","    def build(self, input_shape):\n","        input_dim = input_shape[-1]\n","        self.w = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True, name='kernel')\n","        self.b = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True, name='bias')\n","        # Working memory state (trainable=False means it's updated manually/in the call)\n","        self.context_state = self.add_weight(shape=(self.units,), initializer='ones', trainable=False, name='working_memory_context')\n","        self.built = True\n","    def call(self, inputs):\n","        pre_activation = tf.matmul(inputs, self.w) + self.b\n","        gated_output = pre_activation * self.context_state\n","        output_activity = self.activation(gated_output)\n","\n","        # Context Update Logic (Simulated update based on current batch activity)\n","        decayed_context = self.context_state * self.context_decay\n","        # Use the mean activity of the batch for a stable update signal\n","        mean_activity = tf.reduce_mean(output_activity, axis=0)\n","        new_signal = mean_activity * self.context_learn_rate\n","\n","        self.context_state.assign(decayed_context + new_signal)\n","        return output_activity\n","\n","class InternalPredictorLayer(Layer):\n","    \"\"\"Internal monitoring signal layer (for internal consistency loss).\"\"\"\n","    def __init__(self, units, activation='linear', **kwargs):\n","        super().__init__(**kwargs)\n","        self.units = units\n","        self.activation = tf.keras.activations.get(activation)\n","    def build(self, input_shape):\n","        input_dim = input_shape[-1]\n","        self.w = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True, name='prediction_kernel')\n","        self.b = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True, name='prediction_bias')\n","        self.built = True\n","    def call(self, inputs):\n","        predicted_state = tf.matmul(inputs, self.w) + self.b\n","        return self.activation(predicted_state)\n","\n","# --- Core Logic Function ---\n","def replicate_frontal_lobe_version(input_tensor, output_size):\n","    \"\"\"The unified Frontal Lobe module.\"\"\"\n","    predictor = InternalPredictorLayer(units=output_size, name='internal_predictor_pfc_input')(input_tensor)\n","    gated_output = ExecutiveGatingLayer(units=output_size, name='executive_gating_pfc_output')(input_tensor)\n","    return gated_output, predictor\n","\n","# --- The 21-Stream Model Builder Function ---\n","def build_multimodal_model(image_shape, data_input_size,\n","                           ts_steps, ts_dim, seq_len, seq_dim,\n","                           graph_dim, num_classes):\n","\n","    # Define placeholder variables for the new input shapes\n","    # NOTE: These are now properly defined in Cell 1, but we keep the parameter list\n","\n","    # --- DEFINE ALL 21 INPUTS ---\n","    vision_input = Input(shape=image_shape, name=\"stream_01_image_input\")\n","    data_input = Input(shape=(data_input_size,), name=\"stream_02_structured_data_input\")\n","    ts_input_1 = Input(shape=(ts_steps, ts_dim), name=\"stream_03_lstm_input\")\n","    ts_input_2 = Input(shape=(ts_steps, ts_dim), name=\"stream_04_gru_input\")\n","    ts_input_3 = Input(shape=(ts_steps, ts_dim), name=\"stream_05_bidirectional_input\")\n","    ts_input_4 = Input(shape=(ts_steps, ts_dim), name=\"stream_06_timedistributed_input\")\n","    seq_input_1 = Input(shape=(seq_len,), name=\"stream_07_text_seq_input\")\n","    seq_input_2 = Input(shape=(seq_len, seq_dim), name=\"stream_08_transformer_input\")\n","    grid_input_1 = Input(shape=(32, 32, 1), name=\"stream_09_small_cnn_input\")\n","    grid_input_2 = Input(shape=(64, 64, 3), name=\"stream_10_deep_cnn_input\")\n","    grid_input_3 = Input(shape=(16, 16, 8), name=\"stream_11_conv_autoencoder_input\")\n","    dense_input_1 = Input(shape=(100,), name=\"stream_12_fnn_input_100\")\n","    dense_input_2 = Input(shape=(256,), name=\"stream_13_fnn_input_256\")\n","    dense_input_3 = Input(shape=(128,), name=\"stream_14_vae_latent_input\")\n","    dense_input_4 = Input(shape=(64,), name=\"stream_15_rbm_feature_input\")\n","    complex_input_1 = Input(shape=(graph_dim * graph_dim,), name=\"stream_16_graph_input_flat\")\n","    complex_input_2 = Input(shape=(40,), name=\"stream_17_attention_vector_input\")\n","    complex_input_3 = Input(shape=(80,), name=\"stream_18_custom_encoder_input\")\n","    residual_input_1 = Input(shape=(512,), name=\"stream_19_residual_fwd_input\")\n","    residual_input_2 = Input(shape=(512,), name=\"stream_20_residual_bwd_input\")\n","    residual_input_3 = Input(shape=(512,), name=\"stream_21_residual_final_input\")\n","\n","    all_feature_streams = []\n","\n","    # --- 21 FEATURE PATHWAYS (Standardized to 512 units with HybridSynapseLayer) ---\n","\n","    # 1. VISUAL (ResNet50 for Computer Vision)\n","    base_vision = ResNet50(weights='imagenet', include_top=False, input_tensor=vision_input, pooling='avg')\n","    base_vision.trainable = False\n","    vision_features = base_vision.output\n","    x_vision = HybridSynapseLayer(units=512, name=\"stream_01_vision_synapse_layer\")(vision_features)\n","    x_vision = Dense(512, activation='relu', name='stream_01_vision_final_dense')(x_vision)\n","    all_feature_streams.append(x_vision)\n","\n","    # 2. STRUCTURED DATA (FNN for Data Science)\n","    x_data = Dense(512, activation='relu', name='stream_02_data_dense_1')(data_input)\n","    x_data = HybridSynapseLayer(units=512, name=\"stream_02_data_synapse_layer\")(x_data)\n","    x_data = Dense(512, activation='relu', name='stream_02_data_dense_2')(x_data)\n","    all_feature_streams.append(x_data)\n","\n","    # 3-6. Time-Series/RNN streams\n","    x_ts1 = LSTM(512, return_sequences=False, name='stream_03_lstm_extractor')(ts_input_1)\n","    x_ts1 = HybridSynapseLayer(units=512, name=\"stream_03_ts1_synapse_layer\")(x_ts1)\n","    x_ts1 = Dense(512, activation='relu', name='stream_03_ts1_final_dense')(x_ts1)\n","    all_feature_streams.append(x_ts1)\n","\n","    x_ts2 = GRU(512, name='stream_04_gru_extractor')(ts_input_2)\n","    x_ts2 = HybridSynapseLayer(units=512, name=\"stream_04_ts2_synapse_layer\")(x_ts2)\n","    x_ts2 = Dense(512, activation='relu', name='stream_04_ts2_final_dense')(x_ts2)\n","    all_feature_streams.append(x_ts2)\n","\n","    x_ts3 = tf.keras.layers.Bidirectional(LSTM(256), name='stream_05_bidir_extractor')(ts_input_3)\n","    x_ts3 = Dense(512, activation='relu', name='stream_05_bidir_standardizer')(x_ts3)\n","    x_ts3 = HybridSynapseLayer(units=512, name=\"stream_05_ts3_synapse_layer\")(x_ts3)\n","    x_ts3 = Dense(512, activation='relu', name='stream_05_ts3_final_dense')(x_ts3)\n","    all_feature_streams.append(x_ts3)\n","\n","    x_ts4 = TimeDistributed(Dense(32), name='stream_06_td_processor')(ts_input_4)\n","    x_ts4 = Flatten(name='stream_06_td_flatten')(x_ts4)\n","    x_ts4 = Dense(512, activation='relu', name='stream_06_td_standardizer')(x_ts4)\n","    x_ts4 = HybridSynapseLayer(units=512, name=\"stream_06_ts4_synapse_layer\")(x_ts4)\n","    x_ts4 = Dense(512, activation='relu', name='stream_06_ts4_final_dense')(x_ts4)\n","    all_feature_streams.append(x_ts4)\n","\n","    # 7-8. NLP/Sequential streams\n","    x_seq1 = Embedding(input_dim=1000, output_dim=64, name='stream_07_embedding')(seq_input_1)\n","    x_seq1 = GRU(512, name='stream_07_embedding_gru')(x_seq1)\n","    x_seq1 = HybridSynapseLayer(units=512, name=\"stream_07_seq1_synapse_layer\")(x_seq1)\n","    x_seq1 = Dense(512, activation='relu', name='stream_07_seq1_final_dense')(x_seq1)\n","    all_feature_streams.append(x_seq1)\n","\n","    attention_out = Attention(use_scale=True, name='stream_08_attention')([seq_input_2, seq_input_2])\n","    # 🛑 CRITICAL FIX APPLIED HERE: Replaced tf.reduce_mean with GlobalAveragePooling1D\n","    x_seq2 = GlobalAveragePooling1D(name='stream_08_attention_pool')(attention_out)\n","    x_seq2 = Dense(512, activation='relu', name='stream_08_transformer_standardizer')(x_seq2)\n","    x_seq2 = HybridSynapseLayer(units=512, name=\"stream_08_seq2_synapse_layer\")(x_seq2)\n","    x_seq2 = Dense(512, activation='relu', name='stream_08_seq2_final_dense')(x_seq2)\n","    all_feature_streams.append(x_seq2)\n","\n","    # 9-11. CNN/Grid streams\n","    x_grid1 = Conv2D(32, (3, 3), activation='relu', name='stream_09_conv1')(grid_input_1)\n","    x_grid1 = MaxPooling2D((2, 2), name='stream_09_pool1')(x_grid1)\n","    x_grid1 = Flatten(name='stream_09_flatten')(x_grid1)\n","    x_grid1 = Dense(512, activation='relu', name='stream_09_cnn_standardizer')(x_grid1)\n","    x_grid1 = HybridSynapseLayer(units=512, name=\"stream_09_grid1_synapse_layer\")(x_grid1)\n","    x_grid1 = Dense(512, activation='relu', name='stream_09_grid1_final_dense')(x_grid1)\n","    all_feature_streams.append(x_grid1)\n","\n","    x_grid2 = Conv2D(64, (5, 5), activation='relu', name='stream_10_conv_deep')(grid_input_2)\n","    x_grid2 = Flatten(name='stream_10_flatten_deep')(x_grid2)\n","    x_grid2 = Dense(512, activation='relu', name='stream_10_deep_cnn_standardizer')(x_grid2)\n","    x_grid2 = HybridSynapseLayer(units=512, name=\"stream_10_grid2_synapse_layer\")(x_grid2)\n","    x_grid2 = Dense(512, activation='relu', name='stream_10_grid2_final_dense')(x_grid2)\n","    all_feature_streams.append(x_grid2)\n","\n","    # Encoder output (from a Generative AI model like an Autoencoder)\n","    x_grid3 = Flatten(name='stream_11_flatten')(grid_input_3)\n","    x_grid3 = Dense(512, activation='relu', name='stream_11_autoencoder_input_proj')(x_grid3)\n","    x_grid3 = HybridSynapseLayer(units=512, name=\"stream_11_grid3_synapse_layer\")(x_grid3)\n","    x_grid3 = Dense(512, activation='relu', name='stream_11_grid3_final_dense')(x_grid3)\n","    all_feature_streams.append(x_grid3)\n","\n","    # 12-15. Dense/Vector streams (VAE/RBM for Generative AI/Deep Learning)\n","    x_dense1 = Dense(1024, activation='relu', name='stream_12_dense_1')(dense_input_1)\n","    x_dense1 = Dense(512, activation='relu', name='stream_12_fnn_standardizer')(x_dense1)\n","    x_dense1 = HybridSynapseLayer(units=512, name=\"stream_12_dense1_synapse_layer\")(x_dense1)\n","    x_dense1 = Dense(512, activation='relu', name='stream_12_dense1_final_dense')(x_dense1)\n","    all_feature_streams.append(x_dense1)\n","\n","    x_dense2 = Dense(512, activation='relu', name='stream_13_dense_1')(dense_input_2)\n","    x_dense2 = HybridSynapseLayer(units=512, name=\"stream_13_dense2_synapse_layer\")(x_dense2)\n","    x_dense2 = Dense(512, activation='relu', name='stream_13_dense2_final_dense')(x_dense2)\n","    all_feature_streams.append(x_dense2)\n","\n","    x_dense3 = Dense(512, activation='relu', name='stream_14_vae_proj')(dense_input_3)\n","    x_dense3 = HybridSynapseLayer(units=512, name=\"stream_14_dense3_synapse_layer\")(x_dense3)\n","    x_dense3 = Dense(512, activation='relu', name='stream_14_dense3_final_dense')(x_dense3)\n","    all_feature_streams.append(x_dense3)\n","\n","    x_dense4 = Dense(512, activation='relu', name='stream_15_rbm_proj')(dense_input_4)\n","    x_dense4 = HybridSynapseLayer(units=512, name=\"stream_15_dense4_synapse_layer\")(x_dense4)\n","    x_dense4 = Dense(512, activation='relu', name='stream_15_dense4_final_dense')(x_dense4)\n","    all_feature_streams.append(x_dense4)\n","\n","    # 16-18. Complex streams (Graph/Attention)\n","    x_complex1 = Dense(1024, activation='relu', name='stream_16_graph_dense_1')(complex_input_1)\n","    x_complex1 = Dense(512, activation='relu', name='stream_16_graph_standardizer')(x_complex1)\n","    x_complex1 = HybridSynapseLayer(units=512, name=\"stream_16_complex1_synapse_layer\")(x_complex1)\n","    x_complex1 = Dense(512, activation='relu', name='stream_16_complex1_final_dense')(x_complex1)\n","    all_feature_streams.append(x_complex1)\n","\n","    x_complex2 = Dense(512, activation='relu', name='stream_17_attention_proj')(complex_input_2)\n","    x_complex2 = HybridSynapseLayer(units=512, name=\"stream_17_complex2_synapse_layer\")(x_complex2)\n","    x_complex2 = Dense(512, activation='relu', name='stream_17_complex2_final_dense')(x_complex2)\n","    all_feature_streams.append(x_complex2)\n","\n","    x_complex3 = Dense(512, activation='relu', name='stream_18_custom_encoder_proj')(complex_input_3)\n","    x_complex3 = HybridSynapseLayer(units=512, name=\"stream_18_complex3_synapse_layer\")(x_complex3)\n","    x_complex3 = Dense(512, activation='relu', name='stream_18_complex3_final_dense')(x_complex3)\n","    all_feature_streams.append(x_complex3)\n","\n","    # 19-21. Residual Fused streams\n","    x_res1 = HybridSynapseLayer(units=512, name=\"stream_19_res1_synapse_layer\")(residual_input_1)\n","    # Using tf.keras.layers.Add is correct and Keras-compatible\n","    x_res1 = tf.keras.layers.Add(name='stream_19_residual_add_forward')([x_res1, residual_input_1])\n","    x_res1 = Dense(512, activation='relu', name='stream_19_res1_final_dense')(x_res1)\n","    all_feature_streams.append(x_res1)\n","\n","    x_res2 = HybridSynapseLayer(units=512, name=\"stream_20_res2_synapse_layer\")(residual_input_2)\n","    x_res2 = tf.keras.layers.Add(name='stream_20_residual_add_backward')([x_res2, residual_input_2])\n","    x_res2 = Dense(512, activation='relu', name='stream_20_res2_final_dense')(x_res2)\n","    all_feature_streams.append(x_res2)\n","\n","    x_res3 = HybridSynapseLayer(units=512, name=\"stream_21_res3_synapse_layer\")(residual_input_3)\n","    x_res3 = tf.keras.layers.Add(name='stream_21_residual_add_final')([x_res3, residual_input_3])\n","    x_res3 = Dense(512, activation='relu', name='stream_21_res3_final_dense')(x_res3)\n","    all_feature_streams.append(x_res3)\n","\n","    # --- FINAL FUSION AND HYBRID CORE ---\n","    combined_features = Concatenate(name=\"multimodal_fusion\")(all_feature_streams)\n","\n","    core_features = Dense(512, activation='relu', name='core_dense_pre_custom')(combined_features)\n","    # Apply Hybrid Synapse logic to the central feature fusion block\n","    core_synapse_output = HybridSynapseLayer(units=512, name=\"core_synapse_layer\")(core_features)\n","\n","    # Apply Frontal Lobe Logic\n","    gated_output, predictor_output = replicate_frontal_lobe_version(core_synapse_output, FRONTAL_LOBE_UNITS)\n","\n","    final_output = Dense(num_classes, activation='softmax', name=\"main_classification_output\")(gated_output)\n","\n","    # Model definition with ALL 21 inputs:\n","    model = Model(inputs=[vision_input, data_input, ts_input_1, ts_input_2, ts_input_3, ts_input_4,\n","                          seq_input_1, seq_input_2, grid_input_1, grid_input_2, grid_input_3,\n","                          dense_input_1, dense_input_2, dense_input_3, dense_input_4,\n","                          complex_input_1, complex_input_2, complex_input_3,\n","                          residual_input_1, residual_input_2, residual_input_3],\n","                  outputs=[final_output, predictor_output])\n","    return model"],"metadata":{"id":"G8rzXqV3oje8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 75. Advanced Multimodal AI Project: Custom Training & Simultaneous Update\n","\n","# =========================================================\n","# CUSTOM TRAINING LOOP (Simultaneous Logic with ID() Fix)\n","# =========================================================\n","\n","class HybridMultiModalModel(tf.keras.Model):\n","    \"\"\"Custom model for managing all losses and the simultaneous hybrid weight update.\"\"\"\n","    def __init__(self, multimodal_core_model):\n","        super().__init__()\n","        self.multimodal_core = multimodal_core_model\n","        self.hybrid_layers = [l for l in multimodal_core_model.layers if isinstance(l, HybridSynapseLayer)]\n","        self.internal_loss_fn = tf.keras.losses.MeanSquaredError(name='internal_loss')\n","\n","    def compile(self, optimizer, loss, metrics=None):\n","        # NOTE: Your classification loss is passed here, but the internal loss is calculated manually.\n","        super().compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","\n","    @tf.function\n","    def train_step(self, data):\n","        (x_data, y_targets) = data\n","        x_inputs = x_data\n","        y_true_classification = y_targets[0]\n","        y_true_internal_feature = y_targets[1]\n","\n","        plasticity_deltas = {}\n","        total_plasticity_mag = tf.constant(0.0)\n","\n","        with tf.GradientTape() as tape:\n","            y_pred_classification, y_pred_internal = self.multimodal_core(x_inputs, training=True)\n","\n","            # 🛑 FIX 1: Use compute_loss() instead of compiled_loss()\n","            # This requires passing the input data 'x_inputs' as the first argument.\n","            # Old: classification_loss = self.compiled_loss(y_true_classification, y_pred_classification, regularization_losses=self.losses)\n","            classification_loss = self.compute_loss(\n","                x_inputs, y_true_classification, y_pred_classification, regularization_losses=self.losses\n","            )\n","\n","            internal_consistency_loss = self.internal_loss_fn(y_true_internal_feature, y_pred_internal)\n","            total_loss = classification_loss + (0.1 * internal_consistency_loss)\n","\n","        trainable_vars = self.trainable_variables\n","        gradients = tape.gradient(total_loss, trainable_vars)\n","\n","        # 1. Collect Non-Gradient Plasticity Terms (ΔW_Plasticity) - **NO CHANGE HERE**\n","        for layer in self.hybrid_layers:\n","            delta_p, mag_p = layer.calculate_plasticity_change()\n","            plasticity_deltas[id(layer.w)] = delta_p\n","            total_plasticity_mag += mag_p\n","\n","        # 2. Combine Plasticity with Backprop Gradient - **NO CHANGE HERE**\n","        combined_gradients = []\n","        for grad, var in zip(gradients, trainable_vars):\n","            if id(var) in plasticity_deltas:\n","                # Enforce W_new = W_old + ΔW_Backprop + ΔW_Plasticity\n","                # Note: The backprop term 'grad' is actually -ΔW_Backprop, so we subtract\n","                # the plasticity delta (ΔW_Plasticity) to effectively add it to the weight update.\n","                grad = grad - plasticity_deltas[id(var)]\n","\n","            combined_gradients.append(grad)\n","\n","        # 3. Apply the Combined Gradient - **NO CHANGE HERE**\n","        self.optimizer.apply_gradients(zip(combined_gradients, trainable_vars))\n","\n","        # 🛑 FIX 2: Use compute_metrics() instead of compiled_metrics.update_state()\n","        # This requires passing the input data 'x_inputs' as the first argument.\n","        # Old: self.compiled_metrics.update_state(y_true_classification, y_pred_classification)\n","        self.compute_metrics(x_inputs, y_true_classification, y_pred_classification)\n","\n","\n","        # The return value is correctly structured to report all metrics\n","        return {m.name: m.result() for m in self.metrics} | \\\n","               {\"classification_loss\": classification_loss,\n","                \"internal_loss\": internal_consistency_loss,\n","                \"plasticity_update_mag\": total_plasticity_mag}"],"metadata":{"id":"jtAnowNftx7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 75. Advanced Multimodal AI Project: Execution (Cell 5)\n","# =========================================================\n","# MAIN EXECUTION BLOCK (FIXED for 21 Inputs)\n","# =========================================================\n","import numpy as np\n","import tensorflow as tf\n","import traceback\n","from tensorflow.keras.applications import ResNet50 # Re-import just in case\n","\n","# --- System Setup ---\n","def setup_system():\n","    # Placeholder values for build_multimodal_model (actual values are defined in Cell 1)\n","    # The arguments here are just used for the function signature but are not used internally in the fixed build_multimodal_model\n","    multimodal_core = build_multimodal_model(IMAGE_SHAPE, DATA_INPUT_SIZE,\n","                                             TS_STEPS, TS_DIM, SEQ_LEN, SEQ_DIM,\n","                                             GRAPH_DIM, NUM_CLASSES)\n","    hybrid_model = HybridMultiModalModel(multimodal_core)\n","    hybrid_model.compile(\n","        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n","        loss=tf.keras.losses.CategoricalCrossentropy(),\n","        metrics=[tf.keras.metrics.CategoricalAccuracy()]\n","    )\n","    # RuleBasedReasoner and LIFNeuron are already defined in Cell 2\n","    rule_reasoner = RuleBasedReasoner()\n","    snn_neuron = LIFNeuron()\n","    return hybrid_model, multimodal_core, rule_reasoner, snn_neuron\n","\n","# --- Execution ---\n","print(\"\\n--- INITIALIZING PROJECT ---\")\n","hybrid_model, multimodal_core, rule_reasoner, snn_neuron = setup_system()\n","\n","# Dummy Data Setup for Training (Fixed for 21 Streams)\n","N_SAMPLES = 4096\n","print(f\"Creating dummy data for {N_SAMPLES} samples and 21 streams...\")\n","\n","# List of 21 Dummy Inputs\n","X_inputs = [\n","    # 1. Vision\n","    np.random.rand(N_SAMPLES, *IMAGE_SHAPE).astype(np.float32),      # stream_01_image_input\n","    # 2. Structured Data\n","    np.random.rand(N_SAMPLES, DATA_INPUT_SIZE).astype(np.float32),   # stream_02_structured_data_input\n","    # 3-6. Time-Series / RNN\n","    np.random.rand(N_SAMPLES, TS_STEPS, TS_DIM).astype(np.float32),  # stream_03_lstm_input\n","    np.random.rand(N_SAMPLES, TS_STEPS, TS_DIM).astype(np.float32),  # stream_04_gru_input\n","    np.random.rand(N_SAMPLES, TS_STEPS, TS_DIM).astype(np.float32),  # stream_05_bidirectional_input\n","    np.random.rand(N_SAMPLES, TS_STEPS, TS_DIM).astype(np.float32),  # stream_06_timedistributed_input\n","    # 7-8. NLP / Sequential\n","    np.random.randint(0, 1000, size=(N_SAMPLES, SEQ_LEN)),           # stream_07_text_seq_input (Integer indices)\n","    np.random.rand(N_SAMPLES, SEQ_LEN, SEQ_DIM).astype(np.float32),  # stream_08_transformer_input\n","    # 9-11. Grid / CNN\n","    np.random.rand(N_SAMPLES, 32, 32, 1).astype(np.float32),         # stream_09_small_cnn_input\n","    np.random.rand(N_SAMPLES, 64, 64, 3).astype(np.float32),         # stream_10_deep_cnn_input\n","    np.random.rand(N_SAMPLES, 16, 16, 8).astype(np.float32),         # stream_11_conv_autoencoder_input\n","    # 12-15. Dense / Vector\n","    np.random.rand(N_SAMPLES, 100).astype(np.float32),               # stream_12_fnn_input_100\n","    np.random.rand(N_SAMPLES, 256).astype(np.float32),              # stream_13_fnn_input_256\n","    np.random.rand(N_SAMPLES, 128).astype(np.float32),              # stream_14_vae_latent_input\n","    np.random.rand(N_SAMPLES, 64).astype(np.float32),                # stream_15_rbm_feature_input\n","    # 16-18. Complex / Graph\n","    np.random.rand(N_SAMPLES, GRAPH_DIM * GRAPH_DIM).astype(np.float32), # stream_16_graph_input_flat\n","    np.random.rand(N_SAMPLES, 40).astype(np.float32),                # stream_17_attention_vector_input\n","    np.random.rand(N_SAMPLES, 80).astype(np.float32),                # stream_18_custom_encoder_input\n","    # 19-21. Residual Fused\n","    np.random.rand(N_SAMPLES, 512).astype(np.float32),               # stream_19_residual_fwd_input\n","    np.random.rand(N_SAMPLES, 512).astype(np.float32),               # stream_20_residual_bwd_input\n","    np.random.rand(N_SAMPLES, 512).astype(np.float32),               # stream_21_residual_final_input\n","]\n","\n","# Targets (Labels for the two model outputs)\n","Y_classification = tf.one_hot(np.random.randint(0, NUM_CLASSES, N_SAMPLES), depth=NUM_CLASSES)\n","Y_internal_feature = np.random.rand(N_SAMPLES, FRONTAL_LOBE_UNITS).astype(np.float32)\n","Y_targets = [Y_classification, Y_internal_feature]\n","\n","# Training Execution\n","print(f\"\\n--- Starting FULL Hybrid MultiModal Training ({N_SAMPLES} Samples, 25 Epochs) ---\")\n","print(\"✅ Simultaneous Hybrid Update Logic is ENFORCED and running on a larger dataset with 21 inputs.\")\n","try:\n","    history = hybrid_model.fit(X_inputs, Y_targets, epochs=25, batch_size=TRAINING_BATCH_SIZE, verbose=1)\n","    print(\"\\nTraining Complete. Hybrid logic is fully functional and dynamically stable.\")\n","except Exception as e:\n","    print(\"\\n\\n----------------------------------------------------\")\n","    print(\"🚨 ERROR CAUGHT DURING MODEL TRAINING 🚨\")\n","    print(\"----------------------------------------------------\")\n","    print(\"Error Type:\", type(e).__name__)\n","    print(\"Error Message:\", e)\n","    print(\"\\n--- Full Traceback ---\")\n","    print(traceback.format_exc())\n","    print(\"----------------------------------------------------\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eFJhGieRfez9","outputId":"bdbc2d7c-423a-4576-86c4-9fa9fdb9042d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- INITIALIZING PROJECT ---\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","Creating dummy data for 4096 samples and 21 streams...\n"]}]},{"cell_type":"code","source":["# @title 75. Advanced Multimodal AI Project: Save Final Weights (CORRECTED)\n","\n","print(\"\\n--- Saving Final Learned Hybrid Weights ---\")\n","\n","# Ensure the hybrid_model object exists and is defined from Cell 5 execution\n","if 'hybrid_model' in locals():\n","    try:\n","        # FIX: Changed the filename to end with .weights.h5 as required by Keras\n","        hybrid_model.multimodal_core.save_weights('final_hybrid_core_weights.weights.h5')\n","\n","        print(f\"✅ Successfully saved hybrid core weights to 'final_hybrid_core_weights.weights.h5'.\")\n","\n","        # --- Optional: Save Rule-Based State (if applicable) ---\n","        final_context_state = hybrid_model.multimodal_core.get_layer('executive_gating_pfc_output').context_state.numpy()\n","        np.save('final_executive_context.npy', final_context_state)\n","        print(f\"✅ Saved final Executive Gating Context State to 'final_executive_context.npy'.\")\n","\n","    except Exception as e:\n","        print(f\"❌ ERROR saving weights: {e}\")\n","        print(\"Make sure Cell 5 finished training successfully.\")\n","else:\n","    print(\"❌ ERROR: 'hybrid_model' not found. Please run Cell 5 first to define and train the model.\")"],"metadata":{"id":"hVBNAA2d20Nm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title 75. Advanced Multimodal AI Project: Full Inference & Hybrid Reasoning (Cell 7)\n","# =========================================================\n","# FULL INFERENCE BLOCK (FIXED for 21 Inputs & Hybrid Logic)\n","# =========================================================\n","import numpy as np\n","import tensorflow as tf\n","\n","# NOTE: This relies on 'hybrid_model', 'rule_reasoner', and 'snn_neuron' from Cell 5\n","\n","print(\"\\n--- Starting Full Hybrid System Inference Test ---\")\n","\n","# --- 1. Prepare Single Sample Data for Inference (Fixed for 21 Streams) ---\n","def prepare_single_sample_inputs():\n","    \"\"\"Generates a single, representative sample across all 21 streams.\"\"\"\n","\n","    # Use the same shapes defined in Cell 1\n","    ts_steps, ts_dim, seq_len, seq_dim, graph_dim = TS_STEPS, TS_DIM, SEQ_LEN, SEQ_DIM, GRAPH_DIM\n","\n","    test_inputs = [\n","        # 1. Vision\n","        np.random.rand(1, *IMAGE_SHAPE).astype(np.float32),      # stream_01_image_input\n","        # 2. Structured Data\n","        np.random.rand(1, DATA_INPUT_SIZE).astype(np.float32),   # stream_02_structured_data_input\n","        # 3-6. Time-Series / RNN\n","        np.random.rand(1, ts_steps, ts_dim).astype(np.float32),  # stream_03_lstm_input\n","        np.random.rand(1, ts_steps, ts_dim).astype(np.float32),  # stream_04_gru_input\n","        np.random.rand(1, ts_steps, ts_dim).astype(np.float32),  # stream_05_bidirectional_input\n","        np.random.rand(1, ts_steps, ts_dim).astype(np.float32),  # stream_06_timedistributed_input\n","        # 7-8. NLP / Sequential\n","        np.random.randint(0, 1000, size=(1, seq_len)),           # stream_07_text_seq_input (Integer indices)\n","        np.random.rand(1, seq_len, seq_dim).astype(np.float32),  # stream_08_transformer_input\n","        # 9-11. Grid / CNN\n","        np.random.rand(1, 32, 32, 1).astype(np.float32),         # stream_09_small_cnn_input\n","        np.random.rand(1, 64, 64, 3).astype(np.float32),         # stream_10_deep_cnn_input\n","        np.random.rand(1, 16, 16, 8).astype(np.float32),         # stream_11_conv_autoencoder_input\n","        # 12-15. Dense / Vector\n","        np.random.rand(1, 100).astype(np.float32),               # stream_12_fnn_input_100\n","        np.random.rand(1, 256).astype(np.float32),              # stream_13_fnn_input_256\n","        np.random.rand(1, 128).astype(np.float32),              # stream_14_vae_latent_input\n","        np.random.rand(1, 64).astype(np.float32),                # stream_15_rbm_feature_input\n","        # 16-18. Complex / Graph\n","        np.random.rand(1, graph_dim * graph_dim).astype(np.float32), # stream_16_graph_input_flat\n","        np.random.rand(1, 40).astype(np.float32),                # stream_17_attention_vector_input\n","        np.random.rand(1, 80).astype(np.float32),                # stream_18_custom_encoder_input\n","        # 19-21. Residual Fused\n","        np.random.rand(1, 512).astype(np.float32),               # stream_19_residual_fwd_input\n","        np.random.rand(1, 512).astype(np.float32),               # stream_20_residual_bwd_input\n","        np.random.rand(1, 512).astype(np.float32),               # stream_21_residual_final_input\n","    ]\n","\n","    # Create a representative text input for the symbolic reasoner (Critical example)\n","    representative_text_input = \"The system is performing optimally and no critical alerts.\"\n","\n","    # Generate a dummy feature vector for the SNN (use a slice of the main feature set)\n","    snn_feature_vector = np.random.rand(30).astype(np.float32)\n","\n","    return test_inputs, representative_text_input, snn_feature_vector\n","\n","# --- 2. Execute Full Hybrid Inference Pipeline ---\n","def run_hybrid_inference(hybrid_model, rule_reasoner, snn_neuron, test_inputs, text_input, snn_features):\n","\n","    # 2.1. Neural Network Prediction\n","    # The core model returns the classification probabilities and the internal PFC features\n","    nn_output, internal_prediction = hybrid_model.multimodal_core.predict(test_inputs, verbose=0)\n","\n","    nn_prob = nn_output[0]\n","    nn_pred_class = np.argmax(nn_prob)\n","\n","    print(f\"\\n[ANN/PFC Core] Raw NN Prediction: Class {nn_pred_class} (Confidence: {np.max(nn_prob):.4f})\")\n","\n","    # 2.2. Spiking Neural Network (SNN) Analysis\n","    # The SNN evaluates a small feature set for a secondary opinion (e.g., vigilance/anomaly score)\n","    snn_spikes = snn_neuron.process_features(snn_features)\n","    snn_threshold = 2 # Example threshold\n","    snn_signal = \"ALERT\" if snn_spikes > snn_threshold else \"NORMAL\"\n","\n","    print(f\"[SNN/Vigilance] Total Spikes: {snn_spikes}. SNN Signal: {snn_signal}.\")\n","\n","    # 2.3. Symbolic AI (Rule-Based Refinement)\n","    # The RuleReasoner takes the NN output and the Symbolic text input (e.g., a command or log message)\n","    reasoning_type, final_class = rule_reasoner.apply_rules(nn_prob, text_input)\n","\n","    print(f\"\\n[Symbolic/Executive] Reasoning Type: {reasoning_type}\")\n","    print(f\"[Symbolic/Executive] Final Decision Class: {final_class}\")\n","\n","    # Check for SNN influence (simple rule: if SNN is ALERT, bump the class up by 1)\n","    if snn_signal == \"ALERT\" and reasoning_type != \"SYMBOLIC_OVERRIDE\":\n","        # The SNN acts as an executive modifier if rules haven't already overridden\n","        final_class = np.minimum(final_class + 1, NUM_CLASSES - 1)\n","        reasoning_type += \" + SNN_MODIFIER\"\n","        print(f\"[SNN Effect] SNN ALERT triggered class modification. New Final Class: {final_class}\")\n","\n","    return final_class, reasoning_type\n","\n","# --- Run the full test ---\n","if 'hybrid_model' in locals() and 'rule_reasoner' in locals() and 'snn_neuron' in locals():\n","\n","    # Create the data for the test run\n","    test_inputs, text_input, snn_features = prepare_single_sample_inputs()\n","\n","    # Run the hybrid pipeline\n","    final_decision_class, final_reasoning = run_hybrid_inference(\n","        hybrid_model, rule_reasoner, snn_neuron, test_inputs, text_input, snn_features\n","    )\n","\n","    print(\"\\n----------------------------------------------------\")\n","    print(\"✅ FULL HYBRID SYSTEM DECISION COMPLETE ✅\")\n","    print(f\"Final Class Decision: {final_decision_class}\")\n","    print(f\"Reasoning Path: {final_reasoning}\")\n","    print(\"----------------------------------------------------\")\n","\n","else:\n","    print(\"❌ ERROR: Required variables ('hybrid_model', 'rule_reasoner', 'snn_neuron') not found.\")\n","    print(\"Please ensure Cell 5 (Execution) and Cell 2 (Non-ANN) ran successfully.\")"],"metadata":{"id":"G5w7OyrUhGhq"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMC2IUg7lHm0OB+SVEpkHzi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}